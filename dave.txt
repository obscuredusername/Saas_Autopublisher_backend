import requests
import json
import time
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import re
from datetime import datetime

class WebContentScraper:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
    
    def search_duckduckgo(self, keyword, country_code='us', language='en', max_results=10):
        """
        Search using DuckDuckGo with country and language parameters
        """
        try:
            # DuckDuckGo search URL with region and language
            search_url = "https://html.duckduckgo.com/html/"
            params = {
                'q': keyword,
                'kl': f'{country_code}-{language}',  # Region-language code
                's': '0'  # Start from first result
            }
            
            response = self.session.get(search_url, params=params, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            links = []
            
            # Extract links from DuckDuckGo results
            for result in soup.find_all('a', class_='result__a'):
                href = result.get('href')
                if href and href.startswith('http'):
                    links.append(href)
                    if len(links) >= max_results:
                        break
            
            return links
            
        except Exception as e:
            print(f"Error searching DuckDuckGo: {e}")
            return []
    
    def get_unique_links(self, links, count=5):
        """
        Get unique links by domain to ensure diversity
        Also handles subdomain variations (e.g., en.wikipedia.org vs fr.wikipedia.org)
        """
        unique_links = []
        seen_base_domains = set()
        
        for link in links:
            try:
                domain = urlparse(link).netloc.lower()
                
                # Extract base domain (e.g., wikipedia.org from en.wikipedia.org)
                domain_parts = domain.split('.')
                if len(domain_parts) >= 2:
                    base_domain = '.'.join(domain_parts[-2:])  # Get last two parts
                else:
                    base_domain = domain
                
                # Skip if we already have a link from this base domain
                if base_domain not in seen_base_domains and self.is_valid_url(link):
                    unique_links.append(link)
                    seen_base_domains.add(base_domain)
                    if len(unique_links) >= count:
                        break
            except:
                continue
        
        return unique_links
    
    def is_valid_url(self, url):
        """
        Check if URL is valid and scrapeable
        """
        try:
            parsed = urlparse(url)
            # Skip social media, video sites, and file downloads
            skip_domains = ['youtube.com', 'facebook.com', 'twitter.com', 'instagram.com', 
                          'linkedin.com', 'tiktok.com', 'pinterest.com']
            skip_extensions = ['.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx']
            
            domain = parsed.netloc.lower()
            path = parsed.path.lower()
            
            if any(skip in domain for skip in skip_domains):
                return False
            if any(path.endswith(ext) for ext in skip_extensions):
                return False
            
            return True
        except:
            return False
    
    def extract_main_content(self, html_content, url):
        """
        Extract main content from HTML using various strategies
        """
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Remove unwanted elements
        for element in soup(['script', 'style', 'nav', 'header', 'footer', 
                           'aside', 'advertisement', 'ads', 'comment']):
            element.decompose()
        
        # Try different content extraction strategies
        content = ""
        title = ""
        
        # Extract title
        title_tag = soup.find('title')
        if title_tag:
            title = title_tag.get_text().strip()
        
        # Strategy 1: Look for common content containers
        content_selectors = [
            'article', 'main', '.content', '#content', '.post', '.entry',
            '.article-body', '.post-content', '.entry-content', 
            '[role="main"]', '.main-content'
        ]
        
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                content = content_elem.get_text(separator=' ', strip=True)
                if len(content) > 200:  # Minimum content length
                    break
        
        # Strategy 2: If no content found, look for largest text block
        if not content or len(content) < 200:
            text_elements = soup.find_all(['p', 'div', 'section'])
            largest_text = ""
            
            for elem in text_elements:
                text = elem.get_text(separator=' ', strip=True)
                if len(text) > len(largest_text):
                    largest_text = text
            
            if len(largest_text) > len(content):
                content = largest_text
        
        # Strategy 3: Fallback to body text
        if not content or len(content) < 100:
            body = soup.find('body')
            if body:
                content = body.get_text(separator=' ', strip=True)
        
        # Clean up content
        content = re.sub(r'\s+', ' ', content).strip()
        
        return {
            'title': title,
            'content': content[:5000],  # Limit content length
            'url': url,
            'content_length': len(content)
        }
    
    def scrape_url(self, url, timeout=15):
        """
        Scrape content from a single URL
        """
        try:
            print(f"Scraping: {url}")
            response = self.session.get(url, timeout=timeout)
            response.raise_for_status()
            
            # Check content type
            content_type = response.headers.get('content-type', '').lower()
            if 'html' not in content_type:
                return None
            
            return self.extract_main_content(response.content, url)
            
        except Exception as e:
            print(f"Error scraping {url}: {e}")
            return None
    
    def scrape_multiple_urls(self, urls, target_count=5, delay=2):
        """
        Scrape multiple URLs with delay between requests
        Ensures exactly target_count successful scrapes by using backup URLs
        """
        results = []
        processed_urls = set()
        
        for i, url in enumerate(urls):
            if len(results) >= target_count:
                break
                
            if url in processed_urls:
                continue
                
            processed_urls.add(url)
            print(f"Processing {len(results)+1}/{target_count}: {url}")
            
            result = self.scrape_url(url)
            if result and result['content_length'] > 100:
                results.append(result)
                print(f"✅ Successfully scraped ({len(results)}/{target_count})")
            else:
                print(f"❌ Failed to scrape or insufficient content")
            
            # Add delay between requests to be respectful
            if i < len(urls) - 1 and len(results) < target_count:
                time.sleep(delay)
        
        return results
    
    def save_to_json(self, data, filename):
        """
        Save scraped data to JSON file
        """
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            print(f"Data saved to {filename}")
            return True
        except Exception as e:
            print(f"Error saving to JSON: {e}")
            return False

def main():
    scraper = WebContentScraper()
    
    # Get user input
    print("=== Web Content Scraper ===")
    keyword = input("Enter keyword to search: ").strip()
    country = input("Enter country code (e.g., us, uk, de, fr): ").strip().lower()
    language = input("Enter language code (e.g., en, es, fr, de): ").strip().lower()
    
    if not keyword:
        print("Keyword is required!")
        return
    
    # Set defaults
    if not country:
        country = 'us'
    if not language:
        language = 'en'
    
    print(f"\nSearching for '{keyword}' in country: {country}, language: {language}")
    
    # Search for links
    print("Searching for links...")
    search_results = scraper.search_duckduckgo(keyword, country, language, max_results=25)
    
    if not search_results:
        print("No search results found!")
        return
    
    print(f"Found {len(search_results)} initial results")
    
    # Get unique links (more than 5 to have backups)
    unique_links = scraper.get_unique_links(search_results, count=15)
    
    if len(unique_links) < 5:
        print(f"Warning: Only found {len(unique_links)} unique links")
        if len(unique_links) == 0:
            print("No valid links found!")
            return
    
    print(f"Selected {min(len(unique_links), 15)} unique links (will scrape until we get 5 successful ones):")
    for i, link in enumerate(unique_links[:10], 1):  # Show first 10
        print(f"{i}. {link}")
    if len(unique_links) > 10:
        print(f"... and {len(unique_links) - 10} more backup links")
    
    # Scrape content - keep trying until we get 5 successful scrapes
    print("\nStarting content scraping...")
    scraped_data = scraper.scrape_multiple_urls(unique_links, target_count=5)
    
    if not scraped_data:
        print("No content could be scraped!")
        return
    
    if len(scraped_data) < 5:
        print(f"⚠️  Warning: Only managed to scrape {len(scraped_data)} out of 5 target pages")
        print("This might be due to website restrictions or connection issues")
    
    # Prepare final data structure
    final_data = {
        'search_info': {
            'keyword': keyword,
            'country': country,
            'language': language,
            'timestamp': datetime.now().isoformat(),
            'total_results_found': len(scraped_data)
        },
        'scraped_content': scraped_data
    }
    
    # Save to JSON
    filename = f"scraped_content_{keyword.replace(' ', '_')}_{country}_{language}.json"
    if scraper.save_to_json(final_data, filename):
        print(f"\n✅ Successfully scraped {len(scraped_data)} pages")
        print(f"📁 Data saved as: {filename}")
        
        # Show summary
        print("\n=== Summary ===")
        for i, item in enumerate(scraped_data, 1):
            print(f"{i}. {item['title'][:60]}...")
            print(f"   Content length: {item['content_length']} characters")
            print(f"   URL: {item['url']}")
            print()
    else:
        print("❌ Failed to save data")

if __name__ == "__main__":
    main()